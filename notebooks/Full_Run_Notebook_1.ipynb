{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42b6590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from torch.nn import Linear, Dropout\n",
    "from torch_geometric.data import Dataset, DataLoader\n",
    "from models import SelfAttention, HybridModel\n",
    "from tqdm import tqdm \n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabe86e2",
   "metadata": {},
   "source": [
    "# Load graphs and data features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c674170",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(project='Immunogenicity', entity='kevingivechian')\n",
    "\n",
    "\n",
    "\n",
    "# Argument parser setup\n",
    "parser = argparse.ArgumentParser(description='Train a model on protein graph data')\n",
    "parser.add_argument('--batch_size', type=int, default=60, help='Input batch size for training (default: 60)')\n",
    "parser.add_argument('--lr', type=float, default=0.00001, help='Learning rate (default: 0.00001)')\n",
    "parser.add_argument('--epochs', type=int, default=5, help='epochs (default: 5)')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Now using args to configure your training\n",
    "batch_size = args.batch_size\n",
    "lr = args.lr\n",
    "epochs = args.epochs\n",
    "\n",
    "\n",
    "wandb.config.update({\"batch_size\": batch_size})\n",
    "wandb.config.update({\"learning_rate\": lr})\n",
    "wandb.config.update({\"epochs\": epochs})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " # Import tqdm for the progress bar\n",
    "\n",
    "# Set the directory where your files are located\n",
    "directory = '/gpfs/gibbs/project/krishnaswamy_smita/kbg32/extracted_folder_FULL2/PyGs'\n",
    "\n",
    "# Get a list of all .pt files in the directory\n",
    "files = [f for f in os.listdir(directory) if f.endswith('.pt')]\n",
    "\n",
    "# Initialize an empty list to store the graphs\n",
    "graphs = []\n",
    "\n",
    "# Loop through the files and load each graph, showing a progress bar\n",
    "for file in tqdm(files, desc=\"Loading graphs\"):\n",
    "    file_path = os.path.join(directory, file)\n",
    "    graph = torch.load(file_path)\n",
    "    graphs.append(graph)\n",
    "\n",
    "# Now `graphs` contains all the loaded graph objects\n",
    "print(f\"Loaded {len(graphs)} graphs.\")\n",
    "\n",
    "\n",
    "graphs = [x for x in graphs if 'NXVPMVATV' not in x.name]\n",
    "\n",
    "\n",
    "expanded_df = pd.read_csv('complete_score.csv')\n",
    "expanded_df = pd.read_table('complete_score_Mprops_1_2.csv')\n",
    "expanded_df = expanded_df.dropna(subset='Foreignness_Score')\n",
    "expanded_df['pep_pair'] = expanded_df['peptide'] + expanded_df['allele']\n",
    "\n",
    "\n",
    "\n",
    "data = np.array(expanded_df['Foreignness_Score'].tolist()) # Your U-shaped data\n",
    "data = data.reshape(-1, 1)  # Reshape if the data is 1D\n",
    "\n",
    "qt = QuantileTransformer(n_quantiles=50, random_state=0, output_distribution='normal')\n",
    "transformed_data_q = qt.fit_transform(data)\n",
    "expanded_df['quant_foreign'] = transformed_data_q\n",
    "expanded_df.dropna(subset='quant_foreign')\n",
    "\n",
    "f_dict = dict(zip(expanded_df['peptide'],expanded_df['smoothed_foreign']))\n",
    "fp2_dict = dict(zip(expanded_df['peptide'],expanded_df['Mprop1']))\n",
    "new_imm_dict = dict(zip(expanded_df['peptide'],expanded_df['immunogenicity']))\n",
    "new_imm_dict_pair = dict(zip(expanded_df['pep_pair'],expanded_df['immunogenicity']))\n",
    "\n",
    "expanded_pep_pair = expanded_df['pep_pair'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#cut off h-bonding features for now \n",
    "\n",
    "for data in graphs:  # Assuming data_list is the list containing your graph data\n",
    "    data.x = data.x[:, :-2]\n",
    "\n",
    "\n",
    "\n",
    "hla_df = pd.read_csv('HLA_27_seqs_csv.csv')\n",
    "hla_dict_true = dict(zip(hla_df['allele'], hla_df['seqs']))\n",
    "strings = [x.name for x in graphs]\n",
    "strings = [x for x in strings if 'NXVPMVATV' not in x]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "extracted_substrings = []\n",
    "\n",
    "for s in strings:\n",
    "    start = s.find(\"LPKPLTLR\")\n",
    "    if start != -1:  # Check if the substring exists in the string\n",
    "        end = s.find(\"_\", start)\n",
    "        if end != -1:\n",
    "            substring = s[start:end]\n",
    "            extracted_substrings.append(substring)\n",
    "        else:\n",
    "            print(f\"Underscore not found in string: {s}\")\n",
    "    else:\n",
    "        print(f\"'LPKPLTLR' not found in string: {s}\")\n",
    "\n",
    "        \n",
    "new_values_fp2_values = []\n",
    "new_values_f_values = []\n",
    "\n",
    "new_imm_values = []\n",
    "peptide_order = []\n",
    "\n",
    "\n",
    "# Print the extracted substrings\n",
    "for substring in extracted_substrings:\n",
    "    new_name = substring[8:]\n",
    "    new_values_fp2_values.append(fp2_dict[new_name])\n",
    "    new_values_f_values.append(f_dict[new_name])\n",
    "    peptide_order.append(new_name)\n",
    "    \n",
    "    \n",
    "    \n",
    "for substring in extracted_substrings:\n",
    "    new_name = substring[8:]\n",
    "    new_imm_values.append(new_imm_dict[new_name])\n",
    "\n",
    "\n",
    "\n",
    "hla_name = []\n",
    "\n",
    "for x in graphs:\n",
    "    start_index = 24\n",
    "    end_sequence = \"LPKPLTLR\"\n",
    "    end_index = x.name.find(end_sequence, start_index) + len(end_sequence)\n",
    "    sub_hla = x.name[start_index:end_index] if end_index != -1 else \"\"\n",
    "    hla_name.append(sub_hla)\n",
    "\n",
    "\n",
    "short_strings = hla_name\n",
    "\n",
    "# Update dictionary values with shorter strings\n",
    "for key, value in hla_dict_true.items():\n",
    "    for short_string in short_strings:\n",
    "        if short_string in value:\n",
    "            hla_dict_true[key] = short_string\n",
    "            break  # Stop checking after the first match is found\n",
    "\n",
    "\n",
    "hla_dict = hla_dict_true\n",
    "inverted_hla_dict = {}\n",
    "for key, value in hla_dict.items():\n",
    "    if value in inverted_hla_dict:\n",
    "        inverted_hla_dict[value].append(key)\n",
    "    else:\n",
    "        inverted_hla_dict[value] = [key]\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "corresponding_alleles = []\n",
    "\n",
    "# Loop through each string in hla_name\n",
    "for string in hla_name:\n",
    "    string = string[2:]\n",
    "    found = False\n",
    "    for key in inverted_hla_dict:\n",
    "        if string in key:\n",
    "            corresponding_alleles.append(inverted_hla_dict[key])\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        corresponding_alleles.append([\"Allele not found\"])\n",
    "\n",
    "\n",
    "        \n",
    "scores = new_values_fp2_values\n",
    "\n",
    "\n",
    "\n",
    "for data, score in zip(graphs, scores):\n",
    "    data.y = torch.tensor([score], dtype=torch.float)  # We use a one-element tensor for each graph-level label\n",
    "    data.x = torch.cat([data.x, data.coords], dim=-1)\n",
    "\n",
    "    data.x = data.x.to(dtype=torch.float32)\n",
    "    data.y = data.y.to(dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def pad_graph(graph, max_nodes, feature_size, coord_size):\n",
    "    num_nodes_to_add = max_nodes - graph.num_nodes\n",
    "    if num_nodes_to_add > 0:\n",
    "        # Pad node features\n",
    "        zero_features = torch.zeros(num_nodes_to_add, feature_size)\n",
    "        padded_features = torch.cat([graph.x, zero_features], dim=0)\n",
    "\n",
    "        # Pad coordinates\n",
    "        zero_coords = torch.zeros(num_nodes_to_add, coord_size)\n",
    "        padded_coords = torch.cat([graph.coords, zero_coords], dim=0)\n",
    "\n",
    "        # Update the graph\n",
    "        graph.x = padded_features\n",
    "        graph.coords = padded_coords\n",
    "        graph.num_nodes = max_nodes\n",
    "    return graph\n",
    "\n",
    "# Example usage\n",
    "graphs_to_pad = graphs  # Replace with your list of graphs\n",
    "max_nodes = max(graph.num_nodes for graph in graphs_to_pad)\n",
    "feature_size = 23  # Replace with the size of your feature vectors\n",
    "coord_size = 3     # Replace with the size of your coordinate vectors\n",
    "\n",
    "padded_graphs = [pad_graph(graph, max_nodes, feature_size, coord_size) for graph in graphs_to_pad]\n",
    "\n",
    "\n",
    "\n",
    "graphs = padded_graphs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe426277",
   "metadata": {},
   "source": [
    "# Load and process sequence data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6d9238",
   "metadata": {},
   "outputs": [],
   "source": [
    "####LOAD and PREPROCESS SEQUENCE DATA     \n",
    "    \n",
    "    \n",
    "seq_df = pd.DataFrame(peptide_order)\n",
    "seq_df['protfp2'] = new_values_fp2_values\n",
    "seq_df['f'] = new_values_f_values\n",
    "\n",
    "seq_df.columns = ['peptide','protfp2','f']\n",
    "seq_df['allele'] = corresponding_alleles   \n",
    "    \n",
    "    \n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your existing DataFrame\n",
    "# Assuming expanded_pep_pair is your list of expanded peptide pairs\n",
    "\n",
    "# Function to find the matching allele\n",
    "def find_matching_allele(peptide, alleles, expanded_pep_pair):\n",
    "    for allele in alleles:\n",
    "        combo = peptide + allele\n",
    "        if combo in expanded_pep_pair:\n",
    "            return combo\n",
    "    return 0  # Return None if no match is found\n",
    "\n",
    "# Apply the function to each row\n",
    "seq_df['combo2'] = seq_df.apply(lambda row: find_matching_allele(row['peptide'], row['allele'], expanded_pep_pair), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sums = []\n",
    "z_peps = []\n",
    "\n",
    "for x in seq_df[seq_df['combo2'] == 0]['peptide'].tolist():\n",
    "    ndf = expanded_df[expanded_df['peptide'] == x]\n",
    "    sums.append(ndf['immunogenicity'].sum())\n",
    "    if ndf['immunogenicity'].sum() == 0:\n",
    "        z_peps.append(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "final_imm = []\n",
    "\n",
    "for x in seq_df['combo2'].tolist():\n",
    "    if x in new_imm_dict_pair.keys():\n",
    "        final_imm.append(new_imm_dict_pair[x])\n",
    "    else:\n",
    "        final_imm.append(0)\n",
    "        \n",
    "seq_df['final_immuno'] = final_imm\n",
    "seq_df['length'] = [len(x) for x in seq_df['peptide'].tolist()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assuming seq_df is your DataFrame and graphs is your list of graphs\n",
    "# z_peps is your list of peptides to keep\n",
    "\n",
    "# Create a mask for rows to keep\n",
    "keep_mask = (seq_df['combo2'] != 0) | (seq_df['peptide'].isin(z_peps))\n",
    "\n",
    "# Filter the DataFrame based on the mask\n",
    "filtered_df = seq_df[keep_mask]\n",
    "\n",
    "# Get the indices of the remaining rows\n",
    "remaining_indices = filtered_df.index.tolist()\n",
    "\n",
    "# Filter the graphs list using these indices\n",
    "filtered_graphs = [graphs[i] for i in remaining_indices]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "#Convert PYTORCH GRAPHS TO DGL GRAPHS\n",
    "\n",
    "\n",
    "# Assuming 'graphs' is your list of graph data objects\n",
    "for graph in filtered_graphs:\n",
    "    # The number of edges is half the size of the second dimension of edge_index\n",
    "    num_edges = graph.edge_index.size(1)\n",
    "    \n",
    "    # Create a tensor of ones with the size equal to the number of edges\n",
    "    # Assuming all edges have a single feature, which is set to 1\n",
    "    graph.edge_attr = torch.ones((num_edges, 1))\n",
    "\n",
    "    # Now, each graph object has an edge_attr tensor filled with ones\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "import torch\n",
    "import dgl\n",
    "#from egnn_pytorch import EGNNConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "dgl_filtered_graphs = []\n",
    "for g in filtered_graphs:\n",
    "    # Example PyTorch Geometric graph (replace with your actual graph)\n",
    "    #print(g.name)\n",
    "    pt_geometric_graph = g\n",
    "\n",
    "    # Convert to DGL graph\n",
    "    src, dst = pt_geometric_graph.edge_index\n",
    "    dgl_graph = dgl.graph((src, dst), num_nodes=pt_geometric_graph.num_nodes)\n",
    "    dgl_graph.ndata['x'] = pt_geometric_graph.x  # Node features\n",
    "    dgl_graph.edata['edge_attr'] = pt_geometric_graph.edge_attr  # Edge attributes\n",
    "\n",
    "    # Extracting node features and coordinate features from dgl_graph\n",
    "    node_feat = dgl_graph.ndata['x'][:, :20]  # First 20 features are node features\n",
    "    coord_feat = dgl_graph.ndata['x'][:, 20:]  # Last 3 features are coordinate features\n",
    "\n",
    "    # Using EGNNConv\n",
    "    #conv = EGNNConv(20, 200, 20)  # Adjust dimensions according to your model\n",
    "\n",
    "    # Forward pass through EGNNConv\n",
    "    #h, x = conv(dgl_graph, node_feat, coord_feat)\n",
    "    dgl_filtered_graphs.append(dgl_graph)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Function to pad peptide sequences\n",
    "def pad_peptide_sequence(sequence, max_length=11, padding_char='J'):\n",
    "    # Pad the sequence with the padding character to reach the max length\n",
    "    padded_sequence = sequence.ljust(max_length, padding_char)\n",
    "    return padded_sequence\n",
    "\n",
    "# Example usage\n",
    "peptides2 = filtered_df['peptide'].tolist() # Add your list of peptides here\n",
    "padded_peptides = [pad_peptide_sequence(pep) for pep in peptides2]\n",
    "filtered_df['peptide_padded'] = padded_peptides\n",
    "\n",
    "\n",
    "def one_hot_encode_sequence(sequence, amino_acids, padding_char='J'):\n",
    "    # Create a dictionary mapping each amino acid and padding character to an integer\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(amino_acids + padding_char))\n",
    "\n",
    "    # Initialize the one-hot encoded matrix for the sequence\n",
    "    one_hot_encoded = np.zeros((len(sequence), len(char_to_int)))\n",
    "\n",
    "    # Fill the one-hot encoded matrix with appropriate values\n",
    "    for i, char in enumerate(sequence):\n",
    "        if char in char_to_int:  # Only encode known characters\n",
    "            one_hot_encoded[i, char_to_int[char]] = 1\n",
    "    \n",
    "    return one_hot_encoded\n",
    "\n",
    "# Define the amino acids and padding character\n",
    "amino_acids = 'ACDEFGHIKLMNPQRSTVWY'  # 20 standard amino acids\n",
    "padding_char = 'J'\n",
    "\n",
    "\n",
    "\n",
    "filtered_df\n",
    "filtered_df['length'] = [len(x) for x in filtered_df['peptide'].tolist()]\n",
    "\n",
    "\n",
    "\n",
    "protein_sequences = filtered_df['peptide_padded'].tolist()\n",
    "protein_reg_values = filtered_df['protfp2'].tolist()\n",
    "protein_immuno_values = filtered_df['final_immuno'].tolist()\n",
    "protein_sequences_non_padded = filtered_df['peptide'].tolist()\n",
    "\n",
    "protein_reg_values_f = filtered_df['f'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "encoded_sequences = [one_hot_encode_sequence(seq, amino_acids, padding_char) for seq in protein_sequences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becd514d",
   "metadata": {},
   "source": [
    "# Build Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7575929a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch_geometric.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# Split the graphs into a training set and a validation set\n",
    "#train_graphs, val_graphs = train_test_split(filtered_graphs, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, list_of_graphs, transform=None, pre_transform=None):\n",
    "        super(ProteinDataset, self).__init__('.', transform, pre_transform)\n",
    "        self.list_of_graphs = list_of_graphs\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.list_of_graphs)\n",
    "\n",
    "    def get(self, idx):\n",
    "        # Here we would return the idx-th graph in the list and its corresponding label\n",
    "        data = self.list_of_graphs[idx]\n",
    "        return data\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# Convert the list of NumPy arrays into a single NumPy array\n",
    "encoded_sequences_array = np.array(encoded_sequences)\n",
    "regression_values_array = np.array(protein_reg_values)\n",
    "binary_values_array = np.array(protein_immuno_values)\n",
    "regression_values_array_f = np.array(protein_reg_values_f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch_geometric.data import Dataset, DataLoader\n",
    "\n",
    "# First, split the graphs and sequences into training and non-training sets\n",
    "train_graphs, non_train_graphs = train_test_split(dgl_filtered_graphs, test_size=0.30, random_state=42)  # 30% for non-training\n",
    "\n",
    "\n",
    "X_train, X_non_train, y_train, y_non_train, y_trainf, y_non_trainf = train_test_split(\n",
    "    encoded_sequences_array, regression_values_array, regression_values_array_f, test_size=0.30, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# only run this if 9-mers only is neeeded \n",
    "\n",
    "#train_graphs, non_train_graphs = train_test_split(filtered_graphs9, test_size=0.30, random_state=42)  # 30% for non-training\n",
    "#X_train, X_non_train, y_train, y_non_train = train_test_split(\n",
    "    #encoded_sequences_array, regression_values_array, test_size=0.30, random_state=42\n",
    "#)\n",
    "\n",
    "\n",
    "y_train_b, y_non_train_b = train_test_split(\n",
    "    binary_values_array, test_size=0.30, random_state=42\n",
    ")\n",
    "\n",
    "# Now, split the non-training set into validation and test sets (50% each)\n",
    "val_graphs, test_graphs = train_test_split(non_train_graphs, test_size=0.5, random_state=42)  # 50% of non-training for test\n",
    "\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_non_train, y_non_train, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "X_val, X_test, y_val_f, y_test_f = train_test_split(\n",
    "    X_non_train,y_non_trainf, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_val_b, y_test_b = train_test_split(\n",
    "    y_non_train_b, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Convert the NumPy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor_f = torch.tensor(y_trainf, dtype=torch.float32)\n",
    "y_val_tensor_f = torch.tensor(y_val_f, dtype=torch.float32)\n",
    "y_test_tensor_f = torch.tensor(y_test_f, dtype=torch.float32)\n",
    "\n",
    "\n",
    "y_train_tensor_b = torch.tensor(y_train_b, dtype=torch.float32)\n",
    "y_val_tensor_b = torch.tensor(y_val_b, dtype=torch.float32)\n",
    "y_test_tensor_b = torch.tensor(y_test_b, dtype=torch.float32)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create TensorDatasets (binary)\n",
    "train_dataset_b = TensorDataset(X_train_tensor, y_train_tensor_b)\n",
    "val_dataset_b = TensorDataset(X_val_tensor, y_val_tensor_b)\n",
    "test_dataset_b = TensorDataset(X_test_tensor, y_test_tensor_b)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create DataLoaders (binary)\n",
    "train_loader_b = DataLoader(train_dataset_b, batch_size=batch_size, shuffle=True)\n",
    "val_loader_b = DataLoader(val_dataset_b, batch_size=batch_size, shuffle=False)\n",
    "test_loader_b = DataLoader(test_dataset_b, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# For Graphs\n",
    "train_dataset_g = ProteinDataset(train_graphs)\n",
    "val_dataset_g = ProteinDataset(val_graphs)\n",
    "test_dataset_g = ProteinDataset(test_graphs)\n",
    "\n",
    "train_loader_g = DataLoader(train_dataset_g, batch_size=batch_size, shuffle=True)\n",
    "val_loader_g = DataLoader(val_dataset_g, batch_size=batch_size, shuffle=False)\n",
    "test_loader_g = DataLoader(test_dataset_g, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomGraphDataset(Dataset):\n",
    "    def __init__(self, graphs, sequences, labels, labelsf):\n",
    "        self.graphs = graphs\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.labelsf = labelsf\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphs[idx], self.sequences[idx], self.labels[idx], self.labelsf[idx]\n",
    "\n",
    "\n",
    "def collate(samples):\n",
    "    graphs, seq_data, labels, labelsf = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    seq_data = torch.stack(seq_data, dim=0)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "    labelsf = torch.stack(labelsf, dim=0)\n",
    "    return batched_graph, seq_data, labels, labelsf\n",
    "\n",
    "\n",
    "    \n",
    "train_dataset = CustomGraphDataset(train_graphs, X_train_tensor, y_train_tensor, y_train_tensor_f)\n",
    "val_dataset = CustomGraphDataset(val_graphs, X_val_tensor, y_val_tensor,y_val_tensor_f)\n",
    "test_dataset = CustomGraphDataset(test_graphs, X_test_tensor, y_test_tensor,y_test_tensor_f)\n",
    "\n",
    "train_dataset_b = CustomGraphDataset(train_graphs, X_train_tensor, y_train_tensor_b,y_train_tensor_f)\n",
    "val_dataset_b = CustomGraphDataset(val_graphs, X_val_tensor, y_val_tensor_b,y_val_tensor_f)\n",
    "test_dataset_b = CustomGraphDataset(test_graphs, X_test_tensor, y_test_tensor_b,y_test_tensor_f)\n",
    "\n",
    "\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "\n",
    "train_loader_g = GraphDataLoader(train_dataset, batch_size=batch_size, collate_fn=collate, shuffle=True)\n",
    "val_loader_g = GraphDataLoader(val_dataset, batch_size=batch_size,collate_fn=collate, shuffle=False)\n",
    "test_loader_g = GraphDataLoader(test_dataset, batch_size=batch_size,collate_fn=collate, shuffle=False)\n",
    "#train_loader_g = GraphDataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "train_loader_gb = GraphDataLoader(train_dataset_b, batch_size=batch_size, collate_fn=collate, shuffle=True)\n",
    "val_loader_gb = GraphDataLoader(val_dataset_b, batch_size=batch_size,collate_fn=collate, shuffle=False)\n",
    "test_loader_gb = GraphDataLoader(test_dataset_b, batch_size=batch_size,collate_fn=collate, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082c4905",
   "metadata": {},
   "source": [
    "# Run training loops and log performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93d302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Define the model\n",
    "device = 'cuda'\n",
    "num_epochs = wandb.config.epochs\n",
    "hybrid_model = HybridModel(gat_hidden_channels=100, vae_input_dim=11*21, vae_hidden_dim=400, vae_latent_dim=30, final_output_dim=1)\n",
    "hybrid_model.to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(hybrid_model.parameters(), lr=wandb.config.learning_rate)\n",
    "\n",
    "# Define the loss function\n",
    "def hybrid_loss_function(recon_x, x, mu, logvar, final_output, y):\n",
    "    #MSE = F.mse_loss(recon_x, x, reduction='sum') \n",
    "    MSE = F.mse_loss(recon_x, x.view(-1, 11*21), reduction='sum')\n",
    "\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    regression_loss = F.mse_loss(final_output.squeeze(), y, reduction='sum')\n",
    "    return 0.5*MSE + 0.5*KLD + 2.0*regression_loss\n",
    "\n",
    "# Training loop\n",
    "def train_model():\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        hybrid_model.train()\n",
    "        train_loss = 0\n",
    "        for graph_data, sequence_data, target, peptide_property in train_loader_g:\n",
    "            #single_graph, sequence_data, target = batch[0][0], batch[1][0], batch[2][0]\n",
    "            #print(graph_data)\n",
    "            #print(target)\n",
    "            graph_data = graph_data.to(device)\n",
    "            sequence_data, target, peptide_property = sequence_data.to(device), target.to(device),peptide_property.to(device)\n",
    "            #print(target)\n",
    "            #print(graph_data)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar, final_output = hybrid_model(graph_data, sequence_data, target, peptide_property)\n",
    "            #print('finaloutput')\n",
    "            #print(final_output)\n",
    "            loss = hybrid_loss_function(recon_batch, sequence_data, mu, logvar, final_output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader_g.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation loop\n",
    "        hybrid_model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for graph_data, sequence_data, target, peptide_property in val_loader_g:\n",
    "                graph_data = graph_data.to(device)\n",
    "                sequence_data, target, peptide_property = sequence_data.to(device), target.to(device),peptide_property.to(device)\n",
    "\n",
    "                recon_batch, mu, logvar, final_output = hybrid_model(graph_data, sequence_data,target, peptide_property)\n",
    "                loss = hybrid_loss_function(recon_batch,sequence_data, mu, logvar, final_output, target)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader_g.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Plot the loss curves\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Optionally, run the training loop\n",
    "if __name__ == '__main__':\n",
    "    train_losses, val_losses = train_model()\n",
    "    plot_losses(train_losses, val_losses)\n",
    "    print(\"DONE PRE-TRAINING\")\n",
    "\n",
    "\n",
    "\n",
    "pre_trained = hybrid_model\n",
    "fine_tuned = hybrid_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters for loss terms\n",
    "ALPHA = 5.0  # Weight for the BCE loss (1 or 0 immunogenicity prediciton)\n",
    "BETA = 0.10   # Weight for the MSE loss (reconstruction)\n",
    "THETA = 0.10  # Weight for the KLD loss  \n",
    "\n",
    "\n",
    "\n",
    "# Calculate class weights\n",
    "positive_weight = 13429 /3745\n",
    "class_weights = torch.tensor([1.0, positive_weight], device=device)\n",
    "\n",
    "# Define the loss function with class-weighted BCE\n",
    "def hybrid_loss_function(recon_x, x, mu, logvar, final_output, y, alpha=ALPHA, beta=BETA, theta=THETA):\n",
    "    MSE = F.mse_loss(recon_x, x.view(-1, 11*21), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    # Apply class weights in BCE\n",
    "    #BCE = F.binary_cross_entropy_with_logits(final_output.squeeze(), y, weight=class_weights[y.long()], reduction='sum')\n",
    "    BCE = F.binary_cross_entropy_with_logits(final_output.view(-1), y.view(-1), weight=class_weights[y.long()], reduction='sum')\n",
    "\n",
    "    return beta * MSE + theta * KLD + alpha * BCE\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(fine_tuned.parameters(), lr=wandb.config.learning_rate, weight_decay = 1e-6)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "# Training loop\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "# Training loop\n",
    "def train_model():\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        hybrid_model.train()\n",
    "        train_loss = 0\n",
    "        for graph_data, sequence_data, target, peptide_property in train_loader_gb:\n",
    "            #single_graph, sequence_data, target = batch[0][0], batch[1][0], batch[2][0]\n",
    "            #print(graph_data)\n",
    "            #print(target)\n",
    "            graph_data = graph_data.to(device)\n",
    "            sequence_data, target, peptide_property = sequence_data.to(device), target.to(device),peptide_property.to(device)\n",
    "            #print(target)\n",
    "            #print(graph_data)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar, final_output = hybrid_model(graph_data, sequence_data, target, peptide_property)\n",
    "            #print('finaloutput')\n",
    "            #print(final_output)\n",
    "            loss = hybrid_loss_function(recon_batch, sequence_data, mu, logvar, final_output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader_gb.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation loop\n",
    "        hybrid_model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for graph_data, sequence_data, target, peptide_property in val_loader_gb:\n",
    "                graph_data = graph_data.to(device)\n",
    "                sequence_data, target, peptide_property = sequence_data.to(device), target.to(device),peptide_property.to(device)\n",
    "\n",
    "                recon_batch, mu, logvar, final_output = hybrid_model(graph_data, sequence_data,target, peptide_property)\n",
    "                loss = hybrid_loss_function(recon_batch,sequence_data, mu, logvar, final_output, target)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader_gb.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        wandb.log({\"train_loss\": train_loss})\n",
    "        wandb.log({\"val_loss\": val_loss})\n",
    "\n",
    "    return train_losses, val_losses\n",
    "# Plot the loss curves\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Run the training\n",
    "num_epochs = epochs  # Set the number of epochs\n",
    "#train_losses, val_losses = train_model()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Optionally, run the training loop\n",
    "if __name__ == '__main__':\n",
    "    train_losses, val_losses = train_model()\n",
    "    plot_losses(train_losses, val_losses)\n",
    "    print(\"DONE FINE-TUNING\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "fine_tuned.eval()  # Set the model to evaluation mode\n",
    "\n",
    "true_targets = []\n",
    "predicted_probs = []  # Store raw probabilities for ROC AUC calculation\n",
    "predicted_labels = []  # Store binary predictions for other metrics\n",
    "\n",
    "with torch.no_grad():\n",
    "    for graph_data, sequence_data, target,peptide_property in test_loader_gb:\n",
    "        graph_data = graph_data.to(device)\n",
    "        sequence_data, target, peptide_property = sequence_data.to(device), target.to(device), peptide_property.to(device)\n",
    "\n",
    "        _, _, _, final_output = fine_tuned(graph_data, sequence_data, target,peptide_property)\n",
    "\n",
    "        # Convert to probabilities\n",
    "        probs = torch.sigmoid(final_output).squeeze()\n",
    "\n",
    "        # Handle the case where probs is a scalar\n",
    "        if probs.ndim == 0:\n",
    "            probs = probs.unsqueeze(0)  # Make it a 1-element tensor\n",
    "\n",
    "        probs = probs.cpu().numpy()\n",
    "\n",
    "        # Convert probabilities to binary predictions\n",
    "        predicted = np.round(probs)\n",
    "\n",
    "        true_targets.extend(target.cpu().numpy())\n",
    "        predicted_probs.extend(probs.tolist())  # Convert to list before extending\n",
    "        predicted_labels.extend(predicted)\n",
    "\n",
    "# Calculate metrics\n",
    "true_targets = np.array(true_targets)\n",
    "predicted_probs = np.array(predicted_probs)\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "accuracy = accuracy_score(true_targets, predicted_labels)\n",
    "precision = precision_score(true_targets, predicted_labels)\n",
    "recall = recall_score(true_targets, predicted_labels)\n",
    "f1 = f1_score(true_targets, predicted_labels)\n",
    "roc_auc = roc_auc_score(true_targets, predicted_probs)\n",
    "\n",
    "\n",
    "\n",
    "# Log metrics to Weights & Biases\n",
    "wandb.log({\n",
    "    'Test Accuracy': accuracy,\n",
    "    'Test Precision': precision,\n",
    "    'Test Recall': recall,\n",
    "    'Test F1 Score': f1,\n",
    "    'Test ROC AUC': roc_auc\n",
    "})\n",
    "\n",
    "\n",
    "# Print the metrics\n",
    "print('test_metrics')\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print(f'ROC AUC: {roc_auc:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "true_targets = []\n",
    "predicted_probs = []  # Store raw probabilities for ROC AUC calculation\n",
    "predicted_labels = []  # Store binary predictions for other metrics\n",
    "\n",
    "with torch.no_grad():\n",
    "    for graph_data, sequence_data, target,peptide_property in train_loader_gb:\n",
    "        graph_data = graph_data.to(device)\n",
    "        sequence_data, target, peptide_property = sequence_data.to(device), target.to(device), peptide_property.to(device)\n",
    "\n",
    "        _, _, _, final_output = fine_tuned(graph_data, sequence_data, target,peptide_property)\n",
    "\n",
    "        # Convert to probabilities\n",
    "        probs = torch.sigmoid(final_output).squeeze()\n",
    "\n",
    "        # Handle the case where probs is a scalar\n",
    "        if probs.ndim == 0:\n",
    "            probs = probs.unsqueeze(0)  # Make it a 1-element tensor\n",
    "\n",
    "        probs = probs.cpu().numpy()\n",
    "\n",
    "        # Convert probabilities to binary predictions\n",
    "        predicted = np.round(probs)\n",
    "\n",
    "        true_targets.extend(target.cpu().numpy())\n",
    "        predicted_probs.extend(probs.tolist())  # Convert to list before extending\n",
    "        predicted_labels.extend(predicted)\n",
    "\n",
    "# Calculate metrics\n",
    "true_targets = np.array(true_targets)\n",
    "predicted_probs = np.array(predicted_probs)\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "accuracy = accuracy_score(true_targets, predicted_labels)\n",
    "precision = precision_score(true_targets, predicted_labels)\n",
    "recall = recall_score(true_targets, predicted_labels)\n",
    "f1 = f1_score(true_targets, predicted_labels)\n",
    "roc_auc = roc_auc_score(true_targets, predicted_probs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Log metrics to Weights & Biases\n",
    "wandb.log({\n",
    "    'Train Accuracy': accuracy,\n",
    "    'Train Precision': precision,\n",
    "    'Train Recall': recall,\n",
    "    'Train F1 Score': f1,\n",
    "    'Train ROC AUC': roc_auc\n",
    "})\n",
    "\n",
    "\n",
    "# Print the metrics\n",
    "print('train_metrics')\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print(f'ROC AUC: {roc_auc:.4f}')\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
